{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoangHungLN/MachineLearning_Assignment/blob/main/Extended_Assignment/notebooks/Extended_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dRutvNf1zS0l",
        "outputId": "bbd2acaa-1172-4534-c4bb-41354d63ab85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 06:40:39--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27164412 (26M) [text/plain]\n",
            "Saving to: ‘data/train.json’\n",
            "\n",
            "data/train.json     100%[===================>]  25.91M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-10 06:40:39 (448 MB/s) - ‘data/train.json’ saved [27164412/27164412]\n",
            "\n",
            "--2025-11-10 06:40:39--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3915073 (3.7M) [text/plain]\n",
            "Saving to: ‘data/dev.json’\n",
            "\n",
            "data/dev.json       100%[===================>]   3.73M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2025-11-10 06:40:39 (234 MB/s) - ‘data/dev.json’ saved [3915073/3915073]\n",
            "\n",
            "--2025-11-10 06:40:39--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/modules/forward_algorithm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1590 (1.6K) [text/plain]\n",
            "Saving to: ‘modules/forward_algorithm.py’\n",
            "\n",
            "modules/forward_alg 100%[===================>]   1.55K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-10 06:40:39 (23.8 MB/s) - ‘modules/forward_algorithm.py’ saved [1590/1590]\n",
            "\n",
            "--2025-11-10 06:40:40--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/modules/viterbi_algorithm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2713 (2.6K) [text/plain]\n",
            "Saving to: ‘modules/viterbi_algorithm.py’\n",
            "\n",
            "modules/viterbi_alg 100%[===================>]   2.65K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-10 06:40:40 (39.9 MB/s) - ‘modules/viterbi_algorithm.py’ saved [2713/2713]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "import os\n",
        "import getpass, os, subprocess, textwrap\n",
        "\n",
        "\n",
        "os.makedirs(\"data\", exist_ok = True)\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json -O data/train.json\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json -O data/dev.json\n",
        "os.makedirs(\"modules\", exist_ok= True)\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/modules/forward_algorithm.py -O modules/forward_algorithm.py\n",
        "!wget  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/modules/viterbi_algorithm.py -O modules/viterbi_algorithm.py\n",
        "\n",
        "DATA_FILE = 'data/train.json'\n",
        "DEV_FILE = 'data/dev.json'\n",
        "PARAM_DIR = 'Extended_Assigment/parameters'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_train = json.load(f)\n",
        "with open(DEV_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_dev = json.load(f)\n",
        "\n",
        "TAG_KEY = \"labels\"\n",
        "\n",
        "X_train = [ex['sentence'] for ex in raw_train]\n",
        "Y_train = [ex[TAG_KEY]     for ex in raw_train]\n",
        "X_dev   = [ex['sentence'] for ex in raw_dev]\n",
        "Y_dev   = [ex[TAG_KEY]     for ex in raw_dev]"
      ],
      "metadata": {
        "id": "vfN130uH_WTx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = \"<UNK>\"\n",
        "min_freq = 1\n",
        "\n",
        "word_freq = Counter(w for sent in X_train for w in sent)\n",
        "vocab = {w for w,c in word_freq.items() if c > min_freq}\n",
        "vocab.add(UNK)\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "tag_set = sorted({t for tags in Y_train for t in tags})\n",
        "\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "tag2id  = {t:i for i,t in enumerate(tag_set)}\n",
        "id2tag  = {i:t for t,i in tag2id.items()}\n",
        "\n",
        "def map_unk(sent):\n",
        "    return [w if w in word2id else UNK for w in sent]\n",
        "\n",
        "X_train = [map_unk(s) for s in X_train]\n",
        "X_dev   = [map_unk(s) for s in X_dev]"
      ],
      "metadata": {
        "id": "t_VNNWm4_rJb"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def estimate_hmm_supervised(X, Y, tag2id, word2id,\n",
        "                            alpha_pi=1.0, alpha_A=1.0, alpha_B=1e-3):\n",
        "    K, V = len(tag2id), len(word2id)\n",
        "    pi_cnt = np.full(K,     alpha_pi, dtype=np.float64)\n",
        "    A_cnt  = np.full((K,K), alpha_A,  dtype=np.float64)\n",
        "    B_cnt  = np.full((K,V), alpha_B,  dtype=np.float64)\n",
        "\n",
        "    for words, tags in zip(X, Y):\n",
        "        if not words:\n",
        "            continue\n",
        "        pi_cnt[tag2id[tags[0]]] += 1\n",
        "        for t in range(len(words)):\n",
        "            j = tag2id[tags[t]]\n",
        "            w = word2id[words[t]]\n",
        "            B_cnt[j, w] += 1\n",
        "            if t < len(words)-1:\n",
        "                i = tag2id[tags[t]]\n",
        "                k = tag2id[tags[t+1]]\n",
        "                A_cnt[i, k] += 1\n",
        "\n",
        "    pi = pi_cnt / pi_cnt.sum()\n",
        "    A  = A_cnt / A_cnt.sum(axis=1, keepdims=True)\n",
        "    B  = B_cnt / B_cnt.sum(axis=1, keepdims=True)\n",
        "    return pi, A, B\n",
        "\n",
        "pi, A, B = estimate_hmm_supervised(X_train, Y_train, tag2id, word2id)"
      ],
      "metadata": {
        "id": "Lh4UDXq9_7cA"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(PARAM_DIR, exist_ok=True)\n",
        "\n",
        "np.savez_compressed(\n",
        "    os.path.join(PARAM_DIR, \"hmm_params_supervised.npz\"),\n",
        "    pi=pi, A=A, B=B\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"UNK\": UNK,\n",
        "    \"tag_key\": TAG_KEY,\n",
        "    \"vocab\":   vocab,      # giữ nguyên thứ tự tương ứng với ma trận B\n",
        "    \"tag_set\": tag_set,    # giữ nguyên thứ tự tương ứng với các ma trận\n",
        "    \"alpha\": {\"pi\":1.0, \"A\":1.0, \"B\":1e-3}\n",
        "}\n",
        "with open(os.path.join(PARAM_DIR, \"tag_word_maps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved files:\", os.listdir(PARAM_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTDxjXpUBPPa",
        "outputId": "e3123558-674c-4c0f-b36d-83d98defec42"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved files: ['hmm_params_supervised.npz', 'tag_word_maps.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = tag_set\n",
        "\n",
        "# 4. Chuyển pi, A, B từ array sang dict mà hàm viterbi cần\n",
        "start_p = {\n",
        "    tag: float(pi[i])\n",
        "    for i, tag in enumerate(tag_set)\n",
        "}\n",
        "\n",
        "trans_p = {\n",
        "    tag_i: {\n",
        "        tag_j: float(A[i, j])\n",
        "        for j, tag_j in enumerate(tag_set)\n",
        "    }\n",
        "    for i, tag_i in enumerate(tag_set)\n",
        "}\n",
        "\n",
        "emit_p = {\n",
        "    tag_i: {\n",
        "        word: float(B[i, w])\n",
        "        for w, word in enumerate(vocab)\n",
        "    }\n",
        "    for i, tag_i in enumerate(tag_set)\n",
        "}"
      ],
      "metadata": {
        "id": "4q30LI_Zf6UI"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.viterbi_algorithm import *\n",
        "test_words = X_dev[1]    # đã map UNK rồi trong notebook\n",
        "gold_tags  = Y_dev[1]    # nhãn thật\n",
        "\n",
        "pred_tags, prob = viterbi_algorithm(test_words, states, start_p, trans_p, emit_p)\n",
        "\n",
        "print(\"Câu test:\")\n",
        "print(test_words)\n",
        "print(\"\\nNhãn dự đoán:\")\n",
        "print(pred_tags)\n",
        "print(\"\\nNhãn gold:\")\n",
        "print(gold_tags)\n",
        "print(\"\\nXác suất:\", prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TO1pyqOTgFhb",
        "outputId": "45352115-7545-4ffc-ddb2-f11fe157ec0f"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu test:\n",
            "['The', 'ruling', 'follows', 'a', 'host', 'of', 'problems', 'at', 'Tucson', 'Electric', ',', 'including', 'major', 'write-downs', ',', 'a', '60', '%', 'slash', 'in', 'the', 'common', 'stock', 'dividend', 'and', 'the', 'departure', 'of', 'former', 'Chairman', '<UNK>', '<UNK>', 'during', 'a', 'company', 'investigation', 'of', 'his', 'stock', 'sales', '.']\n",
            "\n",
            "Nhãn dự đoán:\n",
            "['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNP', ',', 'VBG', 'JJ', 'NNS', ',', 'DT', 'CD', 'NN', 'VB', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'NNS', '.']\n",
            "\n",
            "Nhãn gold:\n",
            "['DT', 'NN', 'VBZ', 'DT', 'NN', 'IN', 'NNS', 'IN', 'NNP', 'NNP', ',', 'VBG', 'JJ', 'NNS', ',', 'DT', 'CD', 'NN', 'NN', 'IN', 'DT', 'JJ', 'NN', 'NN', 'CC', 'DT', 'NN', 'IN', 'JJ', 'NNP', 'NNP', 'NNP', 'IN', 'DT', 'NN', 'NN', 'IN', 'PRP$', 'NN', 'NNS', '.']\n",
            "\n",
            "Xác suất: 1.2834557554191526e-108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ed9WSxpaZpW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_sent = [\"he\", \"likes\", \"learning\", \"machine\", \"learning\", \"the\", \"most\"]\n",
        "test_words = map_unk(raw_sent)\n",
        "\n",
        "pred_tags, prob = viterbi(test_words, states, start_p, trans_p, emit_p)\n",
        "\n",
        "print(\"Câu gốc:\", raw_sent)\n",
        "print(\"Câu sau khi map UNK:\", test_words)\n",
        "print(\"Nhãn dự đoán:\", pred_tags)\n",
        "print(\"Xác suất:\", prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0YpqlQngRJi",
        "outputId": "3448faaf-689c-490e-b638-b110d4035db5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Câu gốc: ['he', 'likes', 'learning', 'machine', 'learning', 'the', 'most']\n",
            "Câu sau khi map UNK: ['he', 'likes', 'learning', 'machine', 'learning', 'the', 'most']\n",
            "Nhãn dự đoán: ['PRP', 'VBZ', 'VBG', 'NN', 'VBG', 'DT', 'RBS']\n",
            "Xác suất: 1.15049567357613e-23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.forward_algorithm import *\n",
        "\n",
        "print(\"\\n--- Đang chạy thử Giải thuật Forward ---\")\n",
        "\n",
        "# Chúng ta KHÔNG CẦN TẢI LẠI (load) các file đã lưu,\n",
        "# vì các biến `pi`, `A`, `B`, `X_dev`, `word2id` đã có sẵn trong bộ nhớ.\n",
        "\n",
        "# 2.1. Lấy một câu từ tập dev để làm chuỗi quan sát (O)\n",
        "# (Lưu ý: `X_dev` đã được map_unk ở trên)\n",
        "test_sentence_words = X_dev[1]  # Lấy câu thứ 2 từ tập dev\n",
        "print(f\"Câu kiểm tra: {' '.join(test_sentence_words)}\")\n",
        "\n",
        "# 2.2. Chuyển câu (words) sang chuỗi quan sát O (indices)\n",
        "O_indices = []\n",
        "idx_unk = word2id[UNK] # Lấy chỉ số của token UNK\n",
        "for word in test_sentence_words:\n",
        "    # Lấy chỉ số, nếu không có thì dùng chỉ số UNK\n",
        "    idx = word2id.get(word, idx_unk)\n",
        "    O_indices.append(idx)\n",
        "O_indices = np.array(O_indices)\n",
        "\n",
        "print(f\"Chuỗi quan sát (O) dạng chỉ số: {O_indices[:15]}...\")\n",
        "\n",
        "# 2.3. Gọi hàm forward\n",
        "if len(O_indices) > 0:\n",
        "    total_prob = forward_algorithm(O_indices, pi, A, B)\n",
        "    print(\"\\n--- Kết quả Forward ---\")\n",
        "    print(f\"Tổng xác suất P(O|lambda): {total_prob}\")\n",
        "else:\n",
        "    print(\"Câu kiểm tra bị rỗng, không thể chạy Forward.\")"
      ],
      "metadata": {
        "id": "Gp_d_e9NZncz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4730ade5-6bb0-44f7-c4a1-8ec2ee5f8e9d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Đang chạy thử Giải thuật Forward ---\n",
            "Câu kiểm tra: The ruling follows a host of problems at Tucson Electric , including major write-downs , a 60 % slash in the common stock dividend and the departure of former Chairman <UNK> <UNK> during a company investigation of his stock sales .\n",
            "Chuỗi quan sát (O) dạng chỉ số: [ 8317 19895 14149  9067 15087 17426 18590  9880  8521  3942    26 15329\n",
            " 16460 23114    26]...\n",
            "\n",
            "--- Kết quả Forward ---\n",
            "Tổng xác suất P(O|lambda): 9.834108917689142e-108\n"
          ]
        }
      ]
    }
  ]
}