{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoangHungLN/MachineLearning_Assignment/blob/main/Extended_Assignment/notebooks/Extended_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dRutvNf1zS0l",
        "outputId": "2f4059c4-cd8c-4364-b848-1fe0698e2c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 05:57:35--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27164412 (26M) [text/plain]\n",
            "Saving to: ‘data/train.json’\n",
            "\n",
            "\rdata/train.json       0%[                    ]       0  --.-KB/s               \rdata/train.json     100%[===================>]  25.91M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-10 05:57:35 (228 MB/s) - ‘data/train.json’ saved [27164412/27164412]\n",
            "\n",
            "--2025-11-10 05:57:36--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3915073 (3.7M) [text/plain]\n",
            "Saving to: ‘data/dev.json’\n",
            "\n",
            "data/dev.json       100%[===================>]   3.73M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-11-10 05:57:36 (71.3 MB/s) - ‘data/dev.json’ saved [3915073/3915073]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "import os\n",
        "import getpass, os, subprocess, textwrap\n",
        "\n",
        "\n",
        "os.makedirs(\"data\", exist_ok = True)\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json -O data/train.json\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json -O data/dev.json\n",
        "\n",
        "DATA_FILE = 'data/train.json'\n",
        "DEV_FILE = 'data/dev.json'\n",
        "PARAM_DIR = 'Extended_Assigment/parameters'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_train = json.load(f)\n",
        "with open(DEV_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_dev = json.load(f)\n",
        "\n",
        "TAG_KEY = \"labels\"\n",
        "\n",
        "X_train = [ex['sentence'] for ex in raw_train]\n",
        "Y_train = [ex[TAG_KEY]     for ex in raw_train]\n",
        "X_dev   = [ex['sentence'] for ex in raw_dev]\n",
        "Y_dev   = [ex[TAG_KEY]     for ex in raw_dev]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfN130uH_WTx",
        "outputId": "e291d5cf-b2b1-4419-c685-b1f0e78463d9"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAG_KEY = labels\n",
            "38218 câu train | 5527 câu dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = \"<UNK>\"\n",
        "min_freq = 1\n",
        "\n",
        "word_freq = Counter(w for sent in X_train for w in sent)\n",
        "vocab = {w for w,c in word_freq.items() if c > min_freq}\n",
        "vocab.add(UNK)\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "tag_set = sorted({t for tags in Y_train for t in tags})\n",
        "\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "tag2id  = {t:i for i,t in enumerate(tag_set)}\n",
        "id2tag  = {i:t for t,i in tag2id.items()}\n",
        "\n",
        "def map_unk(sent):\n",
        "    return [w if w in word2id else UNK for w in sent]\n",
        "\n",
        "X_train = [map_unk(s) for s in X_train]\n",
        "X_dev   = [map_unk(s) for s in X_dev]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_VNNWm4_rJb",
        "outputId": "68d3c02b-3c3d-485f-a9eb-ff7b070d8178"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(43194,\n",
              " 45,\n",
              " ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the'],\n",
              " ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT'])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def estimate_hmm_supervised(X, Y, tag2id, word2id,\n",
        "                            alpha_pi=1.0, alpha_A=1.0, alpha_B=1e-3):\n",
        "    K, V = len(tag2id), len(word2id)\n",
        "    pi_cnt = np.full(K,     alpha_pi, dtype=np.float64)\n",
        "    A_cnt  = np.full((K,K), alpha_A,  dtype=np.float64)\n",
        "    B_cnt  = np.full((K,V), alpha_B,  dtype=np.float64)\n",
        "\n",
        "    for words, tags in zip(X, Y):\n",
        "        if not words:\n",
        "            continue\n",
        "        pi_cnt[tag2id[tags[0]]] += 1\n",
        "        for t in range(len(words)):\n",
        "            j = tag2id[tags[t]]\n",
        "            w = word2id[words[t]]\n",
        "            B_cnt[j, w] += 1\n",
        "            if t < len(words)-1:\n",
        "                i = tag2id[tags[t]]\n",
        "                k = tag2id[tags[t+1]]\n",
        "                A_cnt[i, k] += 1\n",
        "\n",
        "    pi = pi_cnt / pi_cnt.sum()\n",
        "    A  = A_cnt / A_cnt.sum(axis=1, keepdims=True)\n",
        "    B  = B_cnt / B_cnt.sum(axis=1, keepdims=True)\n",
        "    return pi, A, B\n",
        "\n",
        "pi, A, B = estimate_hmm_supervised(X_train, Y_train, tag2id, word2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh4UDXq9_7cA",
        "outputId": "bfa40190-5353-4656-a5eb-589778cb5698"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum(pi) = 1.0000000000000002\n",
            "row sums A (5 mẫu) = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "row sums B (5 mẫu) = [1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(PARAM_DIR, exist_ok=True)\n",
        "\n",
        "np.savez_compressed(\n",
        "    os.path.join(PARAM_DIR, \"hmm_params_supervised.npz\"),\n",
        "    pi=pi, A=A, B=B\n",
        ")\n",
        "\n",
        "config = {\n",
        "    \"UNK\": UNK,\n",
        "    \"tag_key\": TAG_KEY,\n",
        "    \"vocab\":   vocab,      # giữ nguyên thứ tự tương ứng với ma trận B\n",
        "    \"tag_set\": tag_set,    # giữ nguyên thứ tự tương ứng với các ma trận\n",
        "    \"alpha\": {\"pi\":1.0, \"A\":1.0, \"B\":1e-3}\n",
        "}\n",
        "with open(os.path.join(PARAM_DIR, \"tag_word_maps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved files:\", os.listdir(PARAM_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTDxjXpUBPPa",
        "outputId": "c0e108e4-42f2-4cd6-eb8c-078d5a2d4405"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved files: ['hmm_params_supervised.npz', 'tag_word_maps.json']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ed9WSxpaZpW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from modules.forward_algorithm import *\n",
        "\n",
        "print(\"\\n--- Đang chạy thử Giải thuật Forward ---\")\n",
        "\n",
        "# Chúng ta KHÔNG CẦN TẢI LẠI (load) các file đã lưu,\n",
        "# vì các biến `pi`, `A`, `B`, `X_dev`, `word2id` đã có sẵn trong bộ nhớ.\n",
        "\n",
        "# 2.1. Lấy một câu từ tập dev để làm chuỗi quan sát (O)\n",
        "# (Lưu ý: `X_dev` đã được map_unk ở trên)\n",
        "test_sentence_words = X_dev[1]  # Lấy câu thứ 2 từ tập dev\n",
        "print(f\"Câu kiểm tra: {' '.join(test_sentence_words)}\")\n",
        "\n",
        "# 2.2. Chuyển câu (words) sang chuỗi quan sát O (indices)\n",
        "O_indices = []\n",
        "idx_unk = word2id[UNK] # Lấy chỉ số của token UNK\n",
        "for word in test_sentence_words:\n",
        "    # Lấy chỉ số, nếu không có thì dùng chỉ số UNK\n",
        "    idx = word2id.get(word, idx_unk)\n",
        "    O_indices.append(idx)\n",
        "O_indices = np.array(O_indices)\n",
        "\n",
        "print(f\"Chuỗi quan sát (O) dạng chỉ số: {O_indices[:15]}...\")\n",
        "\n",
        "# 2.3. Gọi hàm forward\n",
        "if len(O_indices) > 0:\n",
        "    total_prob = forward_algorithm(O_indices, pi, A, B)\n",
        "    print(\"\\n--- Kết quả Forward ---\")\n",
        "    print(f\"Tổng xác suất P(O|lambda): {total_prob}\")\n",
        "else:\n",
        "    print(\"Câu kiểm tra bị rỗng, không thể chạy Forward.\")"
      ],
      "metadata": {
        "id": "Gp_d_e9NZncz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}