{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP1Y7DnLVyyLMygMVQ+a2VZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HoangHungLN/MachineLearning_Assignment/blob/main/Extended_Assignment/notebooks/Extended_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dRutvNf1zS0l",
        "outputId": "fb96aa0c-84bd-41e4-98fc-1d7ed3410eb4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-10 04:28:07--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27164412 (26M) [text/plain]\n",
            "Saving to: ‘data/train.json’\n",
            "\n",
            "data/train.json     100%[===================>]  25.91M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2025-11-10 04:28:08 (208 MB/s) - ‘data/train.json’ saved [27164412/27164412]\n",
            "\n",
            "--2025-11-10 04:28:08--  https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3915073 (3.7M) [text/plain]\n",
            "Saving to: ‘data/dev.json’\n",
            "\n",
            "data/dev.json       100%[===================>]   3.73M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2025-11-10 04:28:08 (64.6 MB/s) - ‘data/dev.json’ saved [3915073/3915073]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter, defaultdict\n",
        "import json\n",
        "import os\n",
        "import getpass, os, subprocess, textwrap\n",
        "\n",
        "\n",
        "os.makedirs(\"data\", exist_ok = True)\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/train.json -O data/train.json\n",
        "!wget https://raw.githubusercontent.com/HoangHungLN/MachineLearning_Assignment/refs/heads/main/Extended_Assignment/data/dev.json -O data/dev.json\n",
        "\n",
        "DATA_FILE = 'data/train.json'\n",
        "DEV_FILE = 'data/dev.json'\n",
        "PARAM_DIR = 'Extended_Assigment/parameters'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw = json.load(f)\n",
        "\n",
        "def build_sentences(raw, use='chunk'):  # use: 'chunk', 'pos', hoặc 'none'\n",
        "    out = []\n",
        "    for ex in raw:\n",
        "        if not isinstance(ex, dict) or 'sentence' not in ex:\n",
        "            continue\n",
        "        words = ex['sentence']\n",
        "\n",
        "        tags = None\n",
        "        if use != 'none':\n",
        "            # ưu tiên đúng khóa nếu có\n",
        "            pref = {'chunk': 'chunk_tags', 'pos': 'pos'}[use]\n",
        "            tags = ex.get(pref) or ex.get('tags') or ex.get('labels')\n",
        "\n",
        "        if isinstance(tags, list) and len(tags) == len(words):\n",
        "            out.append(list(zip(words, tags)))   # [(word, tag), ...]\n",
        "        else:\n",
        "            out.append(words)                    # chỉ word nếu không có tag\n",
        "    return out\n",
        "\n",
        "sentences = build_sentences(raw, use='chunk')   # đổi 'pos' hoặc 'none' tùy bài\n",
        "print(\"Số câu xử lý được:\", len(sentences))\n",
        "print(\"Ví dụ câu đầu:\", sentences[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uezdlbNF3AtI",
        "outputId": "5c9748ec-fdb8-4f79-e57d-52488e4d8755"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Số câu xử lý được: 38218\n",
            "Ví dụ câu đầu: [('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(DATA_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_train = json.load(f)\n",
        "with open(DEV_FILE, 'r', encoding='utf-8') as f:\n",
        "    raw_dev = json.load(f)\n",
        "\n",
        "def detect_tag_key(ex):\n",
        "    for k in ['pos','upos','tags','labels']:\n",
        "        if k in ex and isinstance(ex[k], list) and len(ex[k]) == len(ex['sentence']):\n",
        "            return k\n",
        "    raise ValueError(f\"Không tìm thấy trường nhãn song song với 'sentence'. Keys: {list(ex.keys())}\")\n",
        "\n",
        "TAG_KEY = detect_tag_key(raw_train[0])\n",
        "print(\"TAG_KEY =\", TAG_KEY)\n",
        "\n",
        "X_train = [ex['sentence'] for ex in raw_train]\n",
        "Y_train = [ex[TAG_KEY]     for ex in raw_train]\n",
        "X_dev   = [ex['sentence'] for ex in raw_dev]\n",
        "Y_dev   = [ex[TAG_KEY]     for ex in raw_dev]\n",
        "print(len(X_train), \"câu train |\", len(X_dev), \"câu dev\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfN130uH_WTx",
        "outputId": "44051004-83c2-4571-ac69-6968e4156bec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TAG_KEY = labels\n",
            "38218 câu train | 5527 câu dev\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "UNK = \"<UNK>\"\n",
        "min_freq = 1  # muốn “mạnh tay” hơn thì tăng lên 2,3,...\n",
        "\n",
        "word_freq = Counter(w for sent in X_train for w in sent)\n",
        "vocab = {w for w,c in word_freq.items() if c > min_freq}\n",
        "vocab.add(UNK)\n",
        "vocab = sorted(vocab)\n",
        "\n",
        "tag_set = sorted({t for tags in Y_train for t in tags})\n",
        "\n",
        "word2id = {w:i for i,w in enumerate(vocab)}\n",
        "tag2id  = {t:i for i,t in enumerate(tag_set)}\n",
        "id2tag  = {i:t for t,i in tag2id.items()}\n",
        "\n",
        "def map_unk(sent):\n",
        "    return [w if w in word2id else UNK for w in sent]\n",
        "\n",
        "X_train = [map_unk(s) for s in X_train]\n",
        "X_dev   = [map_unk(s) for s in X_dev]\n",
        "\n",
        "len(vocab), len(tag_set), X_train[0][:10], Y_train[0][:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_VNNWm4_rJb",
        "outputId": "2457414e-5900-4b2e-b311-3adf8b4827d6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(23183,\n",
              " 45,\n",
              " ['Pierre', 'Vinken', ',', '61', 'years', 'old', ',', 'will', 'join', 'the'],\n",
              " ['NNP', 'NNP', ',', 'CD', 'NNS', 'JJ', ',', 'MD', 'VB', 'DT'])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# YÊU CẦU: đã có X_train, Y_train, tag2id, word2id trong bộ nhớ.\n",
        "\n",
        "def estimate_hmm_supervised(X, Y, tag2id, word2id,\n",
        "                            alpha_pi=1.0, alpha_A=1.0, alpha_B=1e-3):\n",
        "    K, V = len(tag2id), len(word2id)\n",
        "    pi_cnt = np.full(K,     alpha_pi, dtype=np.float64)\n",
        "    A_cnt  = np.full((K,K), alpha_A,  dtype=np.float64)\n",
        "    B_cnt  = np.full((K,V), alpha_B,  dtype=np.float64)\n",
        "\n",
        "    for words, tags in zip(X, Y):\n",
        "        if not words:\n",
        "            continue\n",
        "        pi_cnt[tag2id[tags[0]]] += 1\n",
        "        for t in range(len(words)):\n",
        "            j = tag2id[tags[t]]\n",
        "            w = word2id[words[t]]\n",
        "            B_cnt[j, w] += 1\n",
        "            if t < len(words)-1:\n",
        "                i = tag2id[tags[t]]\n",
        "                k = tag2id[tags[t+1]]\n",
        "                A_cnt[i, k] += 1\n",
        "\n",
        "    pi = pi_cnt / pi_cnt.sum()\n",
        "    A  = A_cnt / A_cnt.sum(axis=1, keepdims=True)\n",
        "    B  = B_cnt / B_cnt.sum(axis=1, keepdims=True)\n",
        "    return pi, A, B, np.log(pi), np.log(A), np.log(B)\n",
        "\n",
        "# ƯỚC LƯỢNG\n",
        "pi, A, B, log_pi, log_A, log_B = estimate_hmm_supervised(X_train, Y_train, tag2id, word2id)\n",
        "\n",
        "# KIỂM TRA\n",
        "print(\"sum(pi) =\", float(pi.sum()))\n",
        "print(\"row sums A (5 mẫu) =\", np.round(A.sum(axis=1)[:5], 6).tolist())\n",
        "print(\"row sums B (5 mẫu) =\", np.round(B.sum(axis=1)[:5], 6).tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lh4UDXq9_7cA",
        "outputId": "f67cc028-52e0-4424-a7d4-1ce8dc9fcaec"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum(pi) = 1.0000000000000002\n",
            "row sums A (5 mẫu) = [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "row sums B (5 mẫu) = [1.0, 1.0, 1.0, 1.0, 1.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(PARAM_DIR, exist_ok=True)\n",
        "\n",
        "# 1) Tham số HMM\n",
        "np.savez_compressed(\n",
        "    os.path.join(PARAM_DIR, \"hmm_params_supervised.npz\"),\n",
        "    pi=pi, A=A, B=B,\n",
        "    log_pi=log_pi, log_A=log_A, log_B=log_B\n",
        ")\n",
        "\n",
        "# 2) Ánh xạ và cấu hình để teammate load lại đúng thứ tự\n",
        "config = {\n",
        "    \"UNK\": UNK,\n",
        "    \"tag_key\": TAG_KEY,\n",
        "    \"vocab\":   vocab,      # giữ nguyên thứ tự tương ứng với ma trận B\n",
        "    \"tag_set\": tag_set,    # giữ nguyên thứ tự tương ứng với các ma trận\n",
        "    \"alpha\": {\"pi\":1.0, \"A\":1.0, \"B\":1e-3}\n",
        "}\n",
        "with open(os.path.join(PARAM_DIR, \"tag_word_maps.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Saved files:\", os.listdir(PARAM_DIR))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTDxjXpUBPPa",
        "outputId": "3f1a3744-09a5-474c-bce3-dbc8839399e2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved files: ['hmm_params_supervised.npz', 'tag_word_maps.json']\n"
          ]
        }
      ]
    }
  ]
}